[{"authors":null,"categories":null,"content":"I\u0026rsquo;m a second-year PhD candidate of Northwestern Polytechnicl University (NWPU), advised by professor Yanning Zhang. I obtained my M.S. and B.S. degree in NWPU in 2016 and 2019.\nMy research interests lie in 3D scene perception, including depth estimation, 3D modeling and 6-DoF pose estimation.\n  Download my Curriculum Vitae.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I\u0026rsquo;m a second-year PhD candidate of Northwestern Polytechnicl University (NWPU), advised by professor Yanning Zhang. I obtained my M.S. and B.S. degree in NWPU in 2016 and 2019.\nMy research interests lie in 3D scene perception, including depth estimation, 3D modeling and 6-DoF pose estimation.","tags":null,"title":"Rui Li","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Wowchemy\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://ruili3.github.io/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":["Rui Li","Xiantuo He","Danna Xue","Shaolin Su","Qing Mao","Yu Zhu","Jinqiu Sun","Yanning Zhang"],"categories":null,"content":"","date":1612152000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612152000,"objectID":"e35acade4a83402c3144e3844af7a7a1","permalink":"https://ruili3.github.io/publication/semdepth/","publishdate":"2021-02-01T04:00:00Z","relpermalink":"/publication/semdepth/","section":"publication","summary":"Self-supervised depth estimation has made a great success in learning depth from unlabeled image sequences. While the mappings between image and pixel-wise depth are well-studied in current methods, the correlation between image, depth and scene semantics, however, is less considered. This hinders the network to better understand the real geometry of the scene, since the contextual clues, contribute not only the latent representations of scene depth, but also the straight constraints for depth map. In this paper, we leverage the two benefits by proposing the implicit and explicit semantic guidance for accurate self-supervised depth estimation. We propose a Semantic-aware Spatial Feature Alignment (SSFA) scheme to effectively align implicit semantic features with depth features for scene-aware depth estimation. We also propose a semantic-guided ranking loss to explicitly constrain the estimated depth maps to be consistent with real scene contextual properties. Both semantic label noise and prediction uncertainty is considered to yield reliable depth supervisions. Extensive experimental results show that our method produces high quality depth maps which are consistently superior either on complex scenes or diverse semantic categories, and outperforms the state-of-the-art methods by a significant margin.","tags":null,"title":"Learning Depth via Leveraging Semantics: Self-supervised Monocular Depth Estimation with Both Implicit and Explicit Semantic Guidance","type":"publication"},{"authors":["Rui Li","Qing Mao","Pei Wang","Xiantuo He","Yu Zhu","Jinqiu Sun","Yanning Zhang"],"categories":null,"content":"","date":1608033600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608033600,"objectID":"b76440554b6ee773ff8e2a85e8d8654f","permalink":"https://ruili3.github.io/publication/seedepth/","publishdate":"2020-12-15T12:00:00Z","relpermalink":"/publication/seedepth/","section":"publication","summary":"Self-supervised depth estimation has shown its great effectiveness in producing high quality depth maps given only image sequences as input. However, its performance usually drops when estimating on border areas or objects with thin structures due to the limited depth representation ability. In this paper, we address this problem by proposing a semantic-guided depth representation enhancement method, which promotes both local and global depth feature representations by leveraging rich contextual information. In stead of a single depth network as used in conventional paradigms, we propose an extra semantic segmentation branch to offer extra contextual features for depth estimation. Based on this framework, we enhance the local feature representation by sampling and feeding the point-based features that locate on the semantic edges to an individual Semantic-guided Edge Enhancement module (SEEM), which is specifically designed for promoting depth estimation on the challenging semantic borders. Then, we improve the global feature representation by proposing a semantic-guided multi-level attention mechanism, which enhances the semantic and depth features by exploring pixel-wise correlations in the multi-level depth decoding scheme. Extensive experiments validate the distinct superiority of our method in capturing highly accurate depth on the challenging image areas such as semantic category borders and thin objects. Both quantitative and qualitative experiments on KITTI show that our method outperforms the state-of-the-art methods.","tags":null,"title":"Semantic-Guided Representation Enhancement for Self-supervised Monocular Trained Depth Estimation","type":"publication"},{"authors":null,"categories":null,"content":"Despite the impressive results of self-supervised depth estimation, most current methods leverage only image representations to model image depth, and seldom consider how the semantic information can be utilized. Image semantics are, however, highly coupled with image preferences and scene depth. Therefore itâ€™s necessary to introduce image semantics to assist in the estimation of depth.\nIn this project, we enhance the depth estimation by leveraing the advantages of semantic segmentation task. The semantic branch helps to improve the depth estimation in two aspects. First, the semantic segmentation network generates contextual feature representations, which can be utilized to improve depth representation. Meanwhile, the semantic segmentation provides object masks, with which the depth borders can be further refined. In this project, we explore the feature-level representation enhancement as well as the mask-based depth border refinement.\n Related Papers\n Semantic-Guided Representation Enhancement for Self-supervised Monocular Trained Depth Estimation Learning Depth via Leveraging Semantics: Self-supervised Monocular Depth Estimation with Both Implicit and Explicit Semantic Guidance   ","date":1606780800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606780800,"objectID":"ac0e2c84574b87015bc2e0165deeb3a4","permalink":"https://ruili3.github.io/project/semanticdepth/","publishdate":"2020-12-01T00:00:00Z","relpermalink":"/project/semanticdepth/","section":"project","summary":"In this project, we enhance the self-supervised depth estimation via an auxiliary semantic segmentation task.","tags":null,"title":"Semantic-guided Depth Estimation","type":"project"},{"authors":["Rui Li","Xiantuo He","Yu Zhu","Xianjun Li","Jinqiu Sun","Yanning Zhang"],"categories":null,"content":"","date":1602504000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602504000,"objectID":"40a9fc375a9fae4ba016ae39462f7f43","permalink":"https://ruili3.github.io/publication/depthdce/","publishdate":"2020-10-12T12:00:00Z","relpermalink":"/publication/depthdce/","section":"publication","summary":"Self-supervised depth estimation has shown great prospects in inferring 3D structures using purely unannotated images. However, its performance usually drops when trained on the images with changing brightness and moving objects. In this paper, we address this issue by enhancing the robustness of the self-supervised paradigm using a set of image-based and geometry-based constraints. Our contributions are threefold, 1) we propose a gradient-based robust photometric loss which restrains the false supervisory signals caused by brightness changes, 2) we propose to filter out the unreliable areas that violate the rigid assumption by a novel combined selective mask, which is computed on the forward pass of the network by leveraging the inter-loss consistency and the loss-gradient consistency, and 3) we constrain the motion estimation network to generate across-frame consistent motions via proposing a triplet-based cycle consistency constraint. Extensive experiments conducted on KITTI, Cityscape and Make3D datasets demonstrate the superiority of our method, that the proposed method can effectively handle complex scenes with changing brightness and object motions. Both qualitative and quantitative results show that the proposed method outperforms the state-of-the-art methods.","tags":null,"title":"Enhancing Self-supervised Monocular Depth Estimation via Incorporating Robust Constraints","type":"publication"},{"authors":null,"categories":null,"content":"Self-supervised depth estimation enables to train the network by video sequences, which do not require groundtruth images. However, self-supervised methods are less robust especially under complicated real world dynamic scenes. To be more precise, 1) when trained on images with obvious brightness change, conventional photometric loss will mainly generate the false supervisory signals caused by the brightness difference, instead of generating the right signals caused by image synthesis error, 2) the pixel-wise image corresponding relations in self-supervised methods do not hold in non-static areas of the image. Thus when trained on images under dynamic scenes, false supervisory signals will be produced due to the moving objects such as vehicles or pedestrians, 3) and most self-supervised methods estimate ego-motions only from image pairs, which lead to unstable estimations due to the absence of cross-frame information.\nIn this project, we aim to improve the robustness of self-supervised depth estimation by proposing a set of robust constraints during network traning. The gradient-based photometric loss is proposed to restrain the false supervisory signals generated by brightness changes. A combined selective mask (including the inter-loss mask and the gradient-loss mask) is proposed to filter out unreliable regions containing moving objects. A robust cycle-consistency constraint is proposed, which implicitly facilitate better depth estimation by constraining motion consistency. Experiments show the method outperforms the state-of-the-arts on KITTI dataset.\n Related Papers\n Enhancing Self-supervised Monocular Depth Estimation via Incorporating Robust Constraints   ","date":1594771200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594771200,"objectID":"16f0b5e469f81a7d65f33eca636cad38","permalink":"https://ruili3.github.io/project/robustdepth/","publishdate":"2020-07-15T00:00:00Z","relpermalink":"/project/robustdepth/","section":"project","summary":"In this project, we improve the robustness of self-supervised depth estimation via proposing a set of robust constraints that are unified in the final loss.","tags":null,"title":"Robust Self-supervised Depth Estimation","type":"project"},{"authors":["Rui Li","Dong Gong","Jinqiu Sun","Yu Zhu","Ziwei Wei","Yanning Zhang"],"categories":null,"content":"","date":1571103660,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571103660,"objectID":"612666dccdd5194ce87de38ae7621f5f","permalink":"https://ruili3.github.io/publication/robust-and-accurate-hybrid-structure-from-motion/","publishdate":"2019-10-15T01:41:00Z","relpermalink":"/publication/robust-and-accurate-hybrid-structure-from-motion/","section":"publication","summary":"In this paper, we propose a hybrid Structure-from-Motion scheme which combines the strength of both global and local incremental SfM methods to get a drift-free and accurate estimation with lower time consumption. More specifically, we propose to construct a robust maximum leaf spanning tree (RMLST) from the initial scene graph and further expand it to a robust graph (RG) to grasp the global picture of camera distribution and scene structure. Then the views in the robust graph are solved in global manner as an initial estimation. After that, the remaining views are estimated with the proposed community-based local incremental approach to guarantee local accuracy and scalability. Bundle adjustment is conducted to optimize the estimation. Experiments show that our method is robust and free from the scene drift as global SfM, and shows much better efficiency than incremental approaches. Besides, our algorithm achieves higher accuracy compared with the state-of-the-art methods.","tags":null,"title":"Robust and Accurate Hybrid Structure-From-Motion","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot;\rif porridge == \u0026quot;blueberry\u0026quot;:\rprint(\u0026quot;Eating...\u0026quot;)\r  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}}\r{{% fragment %}} **Two** {{% /fragment %}}\r{{% fragment %}} Three {{% /fragment %}}\r Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}}\r- Only the speaker can read these notes\r- Press `S` key to view\r{{% /speaker_note %}}\r Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}}\r{{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}}\r{{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}\r  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1,\r.reveal section h2,\r.reveal section h3 {\rcolor: navy;\r}\r  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://ruili3.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Rui Li","Jinqiu Sun","Dong Gong","Yu Zhu","Haisen Li","Yanning Zhang"],"categories":null,"content":"","date":1547546220,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547546220,"objectID":"3e713e3c4a15362c8a65b6d2b5bf3d95","permalink":"https://ruili3.github.io/publication/arsac-efficient-model-estimation-via-adaptively-ranked-sample-consensus/","publishdate":"2019-01-15T09:57:00Z","relpermalink":"/publication/arsac-efficient-model-estimation-via-adaptively-ranked-sample-consensus/","section":"publication","summary":"RANSAC is a popular robust model estimation algorithm in various computer vision applications. However, the speed of RANSAC declines dramatically as the inlier rate of the measurements decreases. In this paper, a novel Adaptively Ranked Sample Consensus(ARSAC) algorithm is presented to boost the speed and robustness of RANSAC. The algorithm adopts non-uniform sampling based on the ranked measurements to speed up the sampling process. Instead of a fixed measurement ranking, we design an adaptivescheme which updates the ranking of the measurements, to incorporate high quality measurements into sample at high priority. At the same time, a geometric constraint is proposed during sampling process to select measurements with scattered distribution in images, which could alleviate degenerate cases in epipolar geometry estimation. Experiments on both synthetic and real-world data demonstrate the superiority in efficiency and robustness of the proposed algorithm compared to the state-of-the-art methods.","tags":null,"title":"ARSAC: Efficient Model Estimation via Adaptively Ranked Sample Consensus","type":"publication"},{"authors":null,"categories":null,"content":"Structure-from-motion (SFM) is a longstanding problem in 3D computer vision, it aims to recover 3D structure and camera poses given a set of images as input. Estimating 3D infomation from 2D data is a highly ambiguous problem. Due to the unsettled issues of feature matching as well as the complex estimation procedures, SfM may failed in some challenging scenarios due the lack of robustness. Moreover, it also brings computation burdens when more robust estimation modules is involved.\nIn this project, we aim to address the challenges of structure-from-motion (SFM) in two aspects. Firstly, we focus on the outlier removal procedure in the SFM pipeline, we propose an efficient variant of RANSAC which adaptively incorporate new samples into estimation to improve the convergence speed of robust estimation. Then, we improve the conventional SFM by integrating the advantages of both incremental and global SFM, leading to a new type of robust and accruate hybrid SFM pipeline. Experimental results show the effectiveness of the proposed methods.\n Related Papers\n ARSAC: Efficient Model Estimation via Adaptively Ranked Sample Consensus Robust and Accurate Hybrid Structure-From-Motion   ","date":1544832000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544832000,"objectID":"90dffe5c73876778cb5d60f2fe31aaa1","permalink":"https://ruili3.github.io/project/robustsfm/","publishdate":"2018-12-15T00:00:00Z","relpermalink":"/project/robustsfm/","section":"project","summary":"This project improves the SFM by proposing efficient robust estimation method as well as a robust and accurate hybrid SFM pipeline.","tags":null,"title":"Efficient and Robust Structure-from-Motion","type":"project"},{"authors":["Dong Gong","Rui Li","Yu Zhu","Haisen Li","Jinqiu Sun","Yanning Zhang"],"categories":null,"content":"","date":1527857820,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527857820,"objectID":"9fef409acded6b9293923c846c305ff1","permalink":"https://ruili3.github.io/publication/blind-image-deblurring-by-promoting-group-sparsity/","publishdate":"2018-06-01T12:57:00Z","relpermalink":"/publication/blind-image-deblurring-by-promoting-group-sparsity/","section":"publication","summary":"Blind image deblurring aims to recover the sharp image from a blurred observation, which is an ill-posed inverse problem. Proper image priors for the unknown variables (i.e. latent sharp image and blur kernel) are crucial. Abundant previous methods have shown the effectiveness of the sparsity-based priors on both image gradients and the blur kernel. The correlation among the elements of the sparse variables is paid less attention, however. In this paper, we propose to handle the blind image deblurring problem by promoting group sparsity. The proposed group sparsity priors are based on the fact that the nonzero elements of natural image gradients and blur kernels tend to cluster in structured group pattern. Based on the proposed priors, we introduce proper algorithms to iteratively update latent image gradients and blur kernel, respectively. The proposed algorithms preserve the salient structures and smooth the minor components in image gradients and restrict the blur kernel in a domain of dynamic group sparse vector. To illustrate the reliability of the proposed algorithm, we conduct experiments to analyze the properties of the regularizers and the convergence property of the proposed algorithm. Experiments with both quantitative and visual comparison further prove the effectiveness of the proposed method.","tags":null,"title":"Blind Image Deblurring by Promoting Group Sparsity","type":"publication"}]