<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects | Rui Li</title>
    <link>https://ruili3.github.io/project/</link>
      <atom:link href="https://ruili3.github.io/project/index.xml" rel="self" type="application/rss+xml" />
    <description>Projects</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Tue, 01 Dec 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://ruili3.github.io/images/icon_hu96a6222dada1adccd9e88c2f035c31f6_23489_512x512_fill_lanczos_center_2.png</url>
      <title>Projects</title>
      <link>https://ruili3.github.io/project/</link>
    </image>
    
    <item>
      <title>Semantic-guided Depth Estimation</title>
      <link>https://ruili3.github.io/project/semanticdepth/</link>
      <pubDate>Tue, 01 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://ruili3.github.io/project/semanticdepth/</guid>
      <description>&lt;p&gt;Despite the impressive results of self-supervised depth estimation, most current methods leverage only image representations to model image depth, and seldom consider how the semantic information can be utilized. Image semantics are, however, highly coupled with image preferences and scene depth. Therefore itâ€™s necessary to introduce image semantics to assist in the estimation of depth.&lt;/p&gt;
&lt;p&gt;In this project, we enhance the depth estimation by leveraing the advantages of semantic segmentation task. The semantic branch helps to improve the depth estimation in two aspects. First, the semantic segmentation network generates contextual feature representations, which can be utilized to improve depth representation. Meanwhile, the semantic segmentation provides object masks, with which the depth borders can be further refined. In this project, we explore the feature-level representation enhancement as well as the mask-based depth border refinement.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Related Papers&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;../../publication/SEEDepth/&#34;&gt;Semantic-Guided Representation Enhancement for Self-supervised Monocular Trained Depth Estimation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../../publication/SemDepth/&#34;&gt;Learning Depth via Leveraging Semantics: Self-supervised Monocular Depth Estimation with Both Implicit and Explicit Semantic Guidance&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Robust Self-supervised Depth Estimation</title>
      <link>https://ruili3.github.io/project/robustdepth/</link>
      <pubDate>Wed, 15 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://ruili3.github.io/project/robustdepth/</guid>
      <description>&lt;p&gt;Self-supervised depth estimation enables to train the network by video sequences, which do not require groundtruth images. However, self-supervised methods are less robust especially under complicated real world dynamic scenes. To be more precise, 1) when trained on images with obvious brightness change, conventional photometric loss will mainly generate the false supervisory signals caused by the brightness difference, instead of generating the right signals caused by image synthesis error, 2) the pixel-wise image corresponding relations in self-supervised methods do not hold in non-static areas of the image. Thus when trained on images under
dynamic scenes, false supervisory signals will be produced due to the moving objects such as vehicles or pedestrians, 3) and most self-supervised methods estimate ego-motions only from image pairs, which lead to unstable estimations due to the absence of cross-frame information.&lt;/p&gt;
&lt;p&gt;In this project, we aim to improve the robustness of self-supervised depth estimation by proposing a set of robust constraints during network traning. The gradient-based photometric loss is proposed to restrain the false supervisory signals generated by brightness changes. A combined selective mask (including the inter-loss mask and the gradient-loss mask) is proposed to filter out unreliable regions containing moving objects. A robust cycle-consistency constraint is proposed, which implicitly facilitate better depth estimation by constraining motion consistency. Experiments show the method outperforms the state-of-the-arts on KITTI dataset.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Related Papers&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;../../publication/depthDCE/&#34;&gt;Enhancing Self-supervised Monocular Depth Estimation via Incorporating Robust Constraints&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Efficient and Robust Structure-from-Motion</title>
      <link>https://ruili3.github.io/project/robustsfm/</link>
      <pubDate>Sat, 15 Dec 2018 00:00:00 +0000</pubDate>
      <guid>https://ruili3.github.io/project/robustsfm/</guid>
      <description>&lt;p&gt;Structure-from-motion (SFM) is a longstanding problem in 3D computer vision, it aims to recover 3D structure and camera poses given a set of images as input. Estimating 3D infomation from 2D data is a highly ambiguous problem. Due to the unsettled issues of feature matching as well as the complex estimation procedures, SfM may failed in some challenging scenarios due the lack of robustness. Moreover, it also brings computation burdens when more robust estimation modules is involved.&lt;/p&gt;
&lt;p&gt;In this project, we aim to address the challenges of structure-from-motion (SFM) in two aspects. Firstly, we focus on the outlier removal procedure in the SFM pipeline, we propose an efficient variant of RANSAC which adaptively incorporate new samples into estimation to improve the convergence speed of robust estimation. Then, we improve the conventional SFM by integrating the advantages of both incremental and global SFM, leading to a new type of robust and accruate hybrid SFM pipeline. Experimental results show the effectiveness of the proposed methods.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Related Papers&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;../../publication/arsac-efficient-model-estimation-via-adaptively-ranked-sample-consensus/&#34;&gt;ARSAC: Efficient Model Estimation via Adaptively Ranked Sample Consensus&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../../publication/robust-and-accurate-hybrid-structure-from-motion/&#34;&gt;Robust and Accurate Hybrid Structure-From-Motion&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
  </channel>
</rss>
